{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7444f30-0883-4a87-8e00-c5e5591a2a26",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85832b1-9f31-41e4-b934-cc260e5b3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from numba import cuda\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0259cfd-b37c-4716-aacf-c91d77e41480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())    # cuda 사용 가능 여부 확인\n",
    "print(torch.cuda.device_count())    # 사용 가능한 GPU 개수 확인\n",
    "device = cuda.get_current_device()  # 현재 사용하고 있는 GPU 확인\n",
    "# device.reset()                     # GPU 캐시 리셋\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'    # GPU out of memory error 발생 시 주석 처리\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'    # 사용할 GPU 번호 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')     # cuda 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2d47b-e9da-47e5-9155-cce997e63481",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f50013-6513-44fd-8e48-06dd12ec3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':5,    # 10번 학습\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':1024,\n",
    "    'SEED':41   # 시드 고정\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdbe67-eda2-42ef-bc35-0a2bfd99f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):  # Seed 고정 함수\n",
    "    random.seed(seed)      # random seed 고정\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    # PYTHONHASHSEED 값 설정\n",
    "    np.random.seed(seed)    # numpy seed 고정\n",
    "    torch.manual_seed(seed) # torch seed 고정\n",
    "    torch.cuda.manual_seed(seed)    # torch cuda seed 고정\n",
    "    torch.backends.cudnn.deterministic = True   # torch cudnn seed 고정\n",
    "    torch.backends.cudnn.benchmark = True   # cudnn을 빠르게 하기 위한 옵션으로, 연산 진행시 어떤 알고리즘을 쓸지를 정하는 부분이다.\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정 함수실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68c38e",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b89389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv').drop(columns=['ID', '제품'])    # ID, 제품 컬럼 삭제\n",
    "train_data = train_data[::SKIP_COUNT]\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_keyword_data = pd.read_csv('data/brand_keyword_cnt.csv')\n",
    "brand_keyword_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b387ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_brands = [brand for brand in train_data['브랜드'].unique() if brand not in brand_keyword_data['브랜드'].unique()]\n",
    "num_non_matching_brands = len(non_matching_brands)\n",
    "\n",
    "print(f'Number of non-matching brands: {num_non_matching_brands}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e80a6",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509bd54-9333-4ec7-b197-b70d3c1408ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(dataframe, front_col):\n",
    "    numeric_cols = dataframe.columns[front_col:]\n",
    "    \n",
    "    min_values = dataframe[numeric_cols].min(axis=1)\n",
    "    max_values = dataframe[numeric_cols].max(axis=1)\n",
    "    \n",
    "    ranges = max_values - min_values\n",
    "    ranges[ranges == 0] = 1\n",
    "    \n",
    "    dataframe[numeric_cols] = (dataframe[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
    "    \n",
    "    scale_min_dict = min_values.to_dict()\n",
    "    scale_max_dict = max_values.to_dict()\n",
    "    \n",
    "    return dataframe, scale_max_dict, scale_min_dict\n",
    "\n",
    "train_data, train_scale_max_dict, train_scale_min_dict = min_max_scaling(train_data, 4)\n",
    "brand_keyword_data, brand_keyword_scale_max_dict, brand_keyword_scale_min_dict = min_max_scaling(brand_keyword_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83c51b-f979-4930-9372-f03bdb33abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding 문자형 변수를 숫자로 인코딩\n",
    "label_encoder = LabelEncoder()  # sklearn.preprocessing 패키지의 LabelEncoder() 함수를 사용하여 라벨 인코더 생성\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']  # 문자형 변수를 숫자로 인코딩할 변수명 리스트\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])  # fit 함수를 통해 라벨 인코더 학습\n",
    "    train_data[col] = label_encoder.transform(train_data[col])  # transform 함수를 통해 라벨 인코딩 수행\n",
    "\n",
    "# 브랜드 인코딩\n",
    "label_encoder.fit(brand_keyword_data['브랜드'])\n",
    "brand_keyword_data['브랜드'] = label_encoder.transform(brand_keyword_data['브랜드'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debff992-a1f4-4ade-b2e9-45b234e44412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, front_col, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)    # 제품의 개수\n",
    "    window_size = train_size + predict_size   # 학습 기간 + 예측 기간으로 윈도우사이즈 결정\n",
    "    \n",
    "    #빈 배열 생성\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :front_col]) + 1))   \n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))   \n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :front_col])    # 해당 제품의 앞쪽 4개 열 데이터 [대분류, 중분류, 소분류, 브랜드] encode_info로 저장\n",
    "        sales_data = np.array(data.iloc[i, front_col:])   # 실제 판매량 정보\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j : j + window_size]    # 현재 조합에 해당하는 윈도우 추출\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))   # encode_info를 train_size만큼 반복하여 학습 데이터와 결합한 뒤, 윈도우의 앞쪽 train_size 개의 데이터와 함께 temp_data로 저장\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data   # input_data에 temp_data 저장\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]    # target_data에 윈도우의 뒤쪽 predict_size 개의 데이터 저장\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf39b0f-64f4-4126-9a3d-da5de9f624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, front_col ,train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_rows = len(data)    # 제품의 개수 ( =행의 개수)\n",
    "    \n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :front_col]) + 1))    # 빈 배열 생성\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):   \n",
    "        encode_info = np.array(data.iloc[i, :front_col])    # 해당 제품의 앞쪽 4개 열 데이터 [대분류, 중분류, 소분류, 브랜드] encode_info로 저장\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])   # 실제 판매량 정보\n",
    "        \n",
    "        window = sales_data[-train_size : ]   # 추론을 위한 일별 판매량 기간만큼의 데이터를 window로 저장\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))   # encode_info를 train_size만큼 반복하고, 추론 기간에 해당하는 데이터와 함께 열 방향으로 합쳐서 temp_data를 생성\n",
    "        input_data[i] = temp_data\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c203f18-dfe9-430a-8082-f1143267b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = make_train_data(train_data, 4)\n",
    "train_test_input = make_predict_data(train_data, 4)\n",
    "\n",
    "brand_keyword_input, brand_keyword_target = make_train_data(brand_keyword_data, 1)\n",
    "brand_keyword_test_input = make_predict_data(brand_keyword_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c710abd-1be0-4926-803f-c732d7bffdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Split (고정)\n",
    "# 맨 뒤 데이터의 20%를 Validation Set으로 사용\n",
    "train_data_len = len(train_input)\n",
    "val_train_input = train_input[-int(train_data_len*0.2):]\n",
    "val_train_target = train_target[-int(train_data_len*0.2):]\n",
    "train_input = train_input[:-int(train_data_len*0.2)]\n",
    "train_target = train_target[:-int(train_data_len*0.2)]\n",
    "\n",
    "brand_keyword_data_len = len(brand_keyword_input)\n",
    "val_brand_keyword_input = brand_keyword_input[-int(brand_keyword_data_len*0.2):]\n",
    "val_brand_keyword_target = brand_keyword_target[-int(brand_keyword_data_len*0.2):]\n",
    "brand_keyword_input = brand_keyword_input[:-int(brand_keyword_data_len*0.2)]\n",
    "brand_keyword_target = brand_keyword_target[:-int(brand_keyword_data_len*0.2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be176ad-ccc8-425c-9627-f583c0647489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.shape, train_target.shape, val_train_input.shape, val_train_target.shape, train_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_keyword_input.shape, brand_keyword_target.shape, val_brand_keyword_input.shape, val_brand_keyword_target.shape, brand_keyword_test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f3d76-fcf4-4866-a578-6bb76783bbed",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0a970-4d99-486d-b9b5-210f3cdca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614347b-da14-466f-9d04-b81e5448a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset 은 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한 번에 합니다.\n",
    "모델을 학습할 때, 일반적으로 샘플들을 《미니배치(minibatch)》로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막고,\n",
    "Python의 multiprocessing 을 사용하여 데이터 검색 속도를 높이려고 합니다.\n",
    "DataLoader 는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iterable)입니다.\n",
    "'''\n",
    "# train data\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_train_dataset = CustomDataset(val_train_input, val_train_target)\n",
    "val_train_loader = DataLoader(val_train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# brand keyword data\n",
    "brand_keyword_dataset = CustomDataset(brand_keyword_input, brand_keyword_target)\n",
    "brand_keyword_loader = DataLoader(brand_keyword_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "val_brand_keyword_dataset = CustomDataset(val_brand_keyword_input, val_brand_keyword_target)\n",
    "val_brand_keyword_loader = DataLoader(val_brand_keyword_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f0b66-817d-49ff-9163-a975fb0f239d",
   "metadata": {},
   "source": [
    "### 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927d7ef-e6dd-4ac4-8b89-e3a216c37e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer LSTM\n",
    "class TrainModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(TrainModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)  \n",
    "        self.fc = nn.Sequential(                        # fully connected layer\n",
    "            nn.Linear(hidden_size, hidden_size//2),     # 완전 연결 레이어 1\n",
    "            nn.ReLU(),                            # 활성화 함수\n",
    "            nn.Dropout(),                      # 드롭아웃\n",
    "            nn.Linear(hidden_size//2, output_size),    # 완전 연결 레이어 2\n",
    "        )\n",
    "            \n",
    "        self.actv = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass\n",
    "        # x shape: (B, TRAIN_WINDOW_SIZE, 5)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device) # hidden state 초기화\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, hidden = self.lstm(x, hidden) #LSTM 시행\n",
    "         \n",
    "        # Only use the last output sequence\n",
    "        last_output = lstm_out[:, -1, :]    # LSTM 레이어의 출력 중 마지막 값만 사용\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))    # 마지막 타임스텝의 출력값을 fully connected layer에 통과\n",
    "        \n",
    "        return output.squeeze(1)    # 모델의 출력을 1차원으로 변환\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),    # hidden state 초기화\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))    # cell state 초기화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single hidden layer LSTM\n",
    "class BrandKeywordModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(BrandKeywordModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)  \n",
    "        self.fc = nn.Sequential(                        # fully connected layer\n",
    "            nn.Linear(hidden_size, hidden_size//2),     # 완전 연결 레이어 1\n",
    "            nn.ReLU(),                            # 활성화 함수\n",
    "            nn.Dropout(),                      # 드롭아웃\n",
    "            nn.Linear(hidden_size//2, output_size),    # 완전 연결 레이어 2\n",
    "        )\n",
    "            \n",
    "        self.actv = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass\n",
    "        # x shape: (B, TRAIN_WINDOW_SIZE, 5)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device) # hidden state 초기화\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, hidden = self.lstm(x, hidden) #LSTM 시행\n",
    "         \n",
    "        # Only use the last output sequence\n",
    "        last_output = lstm_out[:, -1, :]    # LSTM 레이어의 출력 중 마지막 값만 사용\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))    # 마지막 타임스텝의 출력값을 fully connected layer에 통과\n",
    "        \n",
    "        return output.squeeze(1)    # 모델의 출력을 1차원으로 변환\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),    # hidden state 초기화\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))    # cell state 초기화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f79f7d",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1802a-35ff-4b43-a1a8-16c8079baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():   # 검증 단계이므로 역전파가 필요없음, no_grad()를 통해 메모리 사용량을 줄임\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73d757-32d5-4868-afbb-1b9f2ea13826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    '''\n",
    "    model : 학습할 모델\n",
    "    optimizer : 최적화 알고리즘\n",
    "    train_loader : 학습 데이터로더\n",
    "    val_loader : 검증 데이터로더\n",
    "    device : 모델 및 데이터 연산에 사용할 디바이스 (GPU 또는 CPU)\n",
    "    '''\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device) # Mean Squared Error\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []     # 학습 데이터셋의 손실값 저장\n",
    "        train_mae = []      # 학습 데이터의 MAE(Mean Absolute Error) 저장\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()   # 이전 iteration에서 계산된 gradient를 초기화\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            loss.backward()        # 역전파로 gradient 계산\n",
    "            optimizer.step()    # 계산된 gradient로 모델의 parameter를 업데이트\n",
    "                        \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if best_loss > val_loss:    # 검증 데이터의 손실값 비교후 가장 적은 모델을 저장\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83fa73-30d5-489c-852b-d655f76a200c",
   "metadata": {},
   "source": [
    "## Run !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1570b00-a309-4e5e-b53d-5848ba53eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainModel()\n",
    "optimizer = torch.optim.Adam(params = train_model.parameters(), lr = CFG[\"LEARNING_RATE\"])    # Adam optimizer를 사용\n",
    "infer_model = train(train_model, optimizer, train_loader, val_train_loader, device)   # 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af56e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_keyword_model = BrandKeywordModel()\n",
    "brand_keyword_optimizer = optim.Adam(brand_keyword_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "brand_keyword_infer_model = train(brand_keyword_model, brand_keyword_optimizer, brand_keyword_loader, val_brand_keyword_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b20af7-f5b1-4a7a-8eb9-7dde5bbf3d04",
   "metadata": {},
   "source": [
    "## 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d7ca0-899e-4515-a43e-890549f8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(train_test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f30d4-2b19-479f-89b7-bf5bb2adc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            output = output.cpu().numpy()       # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
    "            predictions.extend(output)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76e053-6fd2-44a7-8631-d903e7ffa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517978aa-445a-4ece-9217-432682f71230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과를 inverse scaling\n",
    "for idx in range(len(pred)):\n",
    "    pred[idx, :] = pred[idx, :] * (train_scale_max_dict[idx * SKIP_COUNT] - train_scale_min_dict[idx * SKIP_COUNT]) + train_scale_min_dict[idx * SKIP_COUNT]\n",
    "\n",
    "pred = np.round(pred, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa77e-fd03-4539-98fe-563fe2a25121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b50eb-d2d8-4c2d-a5e7-9607220fd794",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c84bb-5dbe-4fb3-aff0-7e229ae29a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db62d9c-b3ad-440a-8cc7-4897b2e4860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.iloc[::SKIP_COUNT,1:] = pred\n",
    "submit = submit[::SKIP_COUNT]\n",
    "submit.to_csv('./data/Ensemble_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
