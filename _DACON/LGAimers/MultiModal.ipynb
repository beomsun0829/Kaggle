{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7444f30-0883-4a87-8e00-c5e5591a2a26",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85832b1-9f31-41e4-b934-cc260e5b3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from numba import cuda\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0259cfd-b37c-4716-aacf-c91d77e41480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())    # cuda 사용 가능 여부 확인\n",
    "print(torch.cuda.device_count())    # 사용 가능한 GPU 개수 확인\n",
    "device = cuda.get_current_device()  # 현재 사용하고 있는 GPU 확인\n",
    "#device.reset()                     # GPU 캐시 리셋\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'    # GPU out of memory error 발생 시 주석 처리\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'    # 사용할 GPU 번호 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')     # cuda 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2d47b-e9da-47e5-9155-cce997e63481",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f50013-6513-44fd-8e48-06dd12ec3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':15,    # 학습횟수\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':256,\n",
    "    'SEED':41   # 시드 고정\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdbe67-eda2-42ef-bc35-0a2bfd99f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):  # Seed 고정 함수\n",
    "    random.seed(seed)      # random seed 고정\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    # PYTHONHASHSEED 값 설정\n",
    "    np.random.seed(seed)    # numpy seed 고정\n",
    "    torch.manual_seed(seed) # torch seed 고정\n",
    "    torch.cuda.manual_seed(seed)    # torch cuda seed 고정\n",
    "    torch.backends.cudnn.deterministic = True   # torch cudnn seed 고정\n",
    "    torch.backends.cudnn.benchmark = True   # cudnn을 빠르게 하기 위한 옵션으로, 연산 진행시 어떤 알고리즘을 쓸지를 정하는 부분이다.\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정 함수실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68c38e",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b89389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sales = pd.read_csv('./data/train.csv').drop(columns=['ID', '제품'])    # ID, 제품 컬럼 삭제\n",
    "train_data_sales = train_data_sales[::SKIP_COUNT]\n",
    "train_data_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c91b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_brand_tmp = pd.read_csv('./data/brand_keyword_cnt.csv')\n",
    "sales_category = train_data_sales[['대분류', '중분류', '소분류', '브랜드']]\n",
    "train_data_brand = pd.merge(sales_category, train_data_brand_tmp, on = '브랜드', how='inner')\n",
    "train_data_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train data brand null count: {train_data_brand.isnull().sum()}')\n",
    "print(f'null index : {train_data_brand[train_data_brand[\"2022-01-01\"].isnull()].index}')\n",
    "train_data_brand.fillna(train_data_brand[\"2022-01-01\"].mean(), inplace=True)\n",
    "print(f'train data brand null count: {train_data_brand.isnull().sum()}')\n",
    "print(f'null index : {train_data_brand[train_data_brand[\"2022-01-01\"].isnull()].index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e80a6",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509bd54-9333-4ec7-b197-b70d3c1408ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_minmax(dataframe):\n",
    "    numeric_cols = dataframe.columns[4:]\n",
    "    # 칵 column의 min 및 max 계산\n",
    "    min_values = dataframe[numeric_cols].min(axis=1)\n",
    "    max_values = dataframe[numeric_cols].max(axis=1)\n",
    "    # 각 행의 범위(max-min)를 계산하고, 범위가 0인 경우 1로 대체\n",
    "    ranges = max_values - min_values\n",
    "    ranges[ranges == 0] = 1\n",
    "    # min-max scaling 수행\n",
    "    dataframe[numeric_cols] = (dataframe[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
    "    # max와 min 값을 dictionary 형태로 저장\n",
    "    scale_min_dict = min_values.to_dict()\n",
    "    scale_max_dict = max_values.to_dict()\n",
    "    \n",
    "    return scale_min_dict, scale_max_dict\n",
    "\n",
    "\n",
    "scale_min_dict_sales, scale_max_dict_sales = scale_minmax(train_data_sales)\n",
    "scale_min_dict_brand, scale_max_dict_brand = scale_minmax(train_data_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83c51b-f979-4930-9372-f03bdb33abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding 문자형 변수를 숫자로 인코딩\n",
    "label_encoder = LabelEncoder()  # sklearn.preprocessing 패키지의 LabelEncoder() 함수를 사용하여 라벨 인코더 생성\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']  # 문자형 변수를 숫자로 인코딩할 변수명 리스트\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data_sales[col])  # fit 함수를 통해 라벨 인코더 학습\n",
    "    train_data_sales[col] = label_encoder.transform(train_data_brand[col])  # transform 함수를 통해 라벨 인코딩 수행\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data_brand[col])\n",
    "    train_data_brand[col] = label_encoder.transform(train_data_brand[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debff992-a1f4-4ade-b2e9-45b234e44412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)    # 제품의 개수\n",
    "    window_size = train_size + predict_size   # 학습 기간 + 예측 기간으로 윈도우사이즈 결정\n",
    "    \n",
    "    #빈 배열 생성\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))   \n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))   \n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])    # 해당 제품의 앞쪽 4개 열 데이터 [대분류, 중분류, 소분류, 브랜드] encode_info로 저장\n",
    "        brand_data = np.array(data.iloc[i, 4:])   # 실제 판매량 정보\n",
    "        \n",
    "        for j in range(len(brand_data) - window_size + 1):\n",
    "            window = brand_data[j : j + window_size]    # 현재 조합에 해당하는 윈도우 추출\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))   # encode_info를 train_size만큼 반복하여 학습 데이터와 결합한 뒤, 윈도우의 앞쪽 train_size 개의 데이터와 함께 temp_data로 저장\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data   # input_data에 temp_data 저장\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]    # target_data에 윈도우의 뒤쪽 predict_size 개의 데이터 저장\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf39b0f-64f4-4126-9a3d-da5de9f624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_rows = len(data)    # 제품의 개수 ( =행의 개수)\n",
    "    \n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))    # 빈 배열 생성\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):   \n",
    "        encode_info = np.array(data.iloc[i, :4])    # 해당 제품의 앞쪽 4개 열 데이터 [대분류, 중분류, 소분류, 브랜드] encode_info로 저장\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])   # 실제 판매량 정보\n",
    "        \n",
    "        window = sales_data[-train_size : ]   # 추론을 위한 일별 판매량 기간만큼의 데이터를 window로 저장\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))   # encode_info를 train_size만큼 반복하고, 추론 기간에 해당하는 데이터와 함께 열 방향으로 합쳐서 temp_data를 생성\n",
    "        input_data[i] = temp_data\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c203f18-dfe9-430a-8082-f1143267b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_sales, train_target_sales = make_train_data(train_data_sales)\n",
    "test_input_sales = make_predict_data(train_data_sales)\n",
    "\n",
    "train_input_brand, train_target_brand = make_train_data(train_data_brand)\n",
    "test_input_brand = make_predict_data(train_data_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c710abd-1be0-4926-803f-c732d7bffdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Split (고정)\n",
    "# 맨 뒤 데이터의 20%를 Validation Set으로 사용\n",
    "data_len_sales = len(train_input_sales)\n",
    "val_input_sales = train_input_sales[-int(data_len_sales*0.2):]\n",
    "val_target_sales = train_target_sales[-int(data_len_sales*0.2):]\n",
    "train_input_sales = train_input_sales[:-int(data_len_sales*0.2)]\n",
    "train_target_sales = train_target_sales[:-int(data_len_sales*0.2)]\n",
    "\n",
    "data_len_brand = len(train_input_brand)\n",
    "val_input_brand = train_input_brand[-int(data_len_brand*0.2):]\n",
    "val_target_brand = train_target_brand[-int(data_len_brand*0.2):]\n",
    "train_input_brand = train_input_brand[:-int(data_len_brand*0.2)]\n",
    "train_target_brand = train_target_brand[:-int(data_len_brand*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be176ad-ccc8-425c-9627-f583c0647489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input_sales.shape, train_target_sales.shape, val_input_sales.shape, val_target_sales.shape, test_input_sales.shape)\n",
    "print(train_input_brand.shape, train_target_brand.shape, val_input_brand.shape, val_target_brand.shape, test_input_brand.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f3d76-fcf4-4866-a578-6bb76783bbed",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0a970-4d99-486d-b9b5-210f3cdca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614347b-da14-466f-9d04-b81e5448a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset 은 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한 번에 합니다.\n",
    "모델을 학습할 때, 일반적으로 샘플들을 《미니배치(minibatch)》로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막고,\n",
    "Python의 multiprocessing 을 사용하여 데이터 검색 속도를 높이려고 합니다.\n",
    "DataLoader 는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iterable)입니다.\n",
    "'''\n",
    "\n",
    "train_dataset_sales = CustomDataset(train_input_sales, train_target_sales)\n",
    "train_loader_sales = DataLoader(train_dataset_sales, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset_sales = CustomDataset(val_input_sales, val_target_sales)\n",
    "val_loader_sales = DataLoader(val_dataset_sales, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "train_dataset_brand = CustomDataset(train_input_brand, train_target_brand)\n",
    "train_loader_brand = DataLoader(train_dataset_brand, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset_brand = CustomDataset(val_input_brand, val_target_brand)\n",
    "val_loader_brand = DataLoader(val_dataset_brand, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f0b66-817d-49ff-9163-a975fb0f239d",
   "metadata": {},
   "source": [
    "### 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927d7ef-e6dd-4ac4-8b89-e3a216c37e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, input_size_brand=5, input_size_sales=5, hidden_size=512, num_layers=4, output_size=CFG[\"PREDICT_SIZE\"]):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm_brand = nn.LSTM(input_size_brand, hidden_size, num_layers = self.num_layers, batch_first=True)\n",
    "        self.lstm_sales = nn.LSTM(input_size_sales, hidden_size, num_layers = self.num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # Concatenated hidden states from brand and sales\n",
    "            nn.Mish(),                                \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        self.actv = nn.Mish()\n",
    "\n",
    "    def forward(self, x_brand, x_sales):\n",
    "        batch_size = x_brand.size(0)\n",
    "\n",
    "        # Brand LSTM\n",
    "        hidden_brand = self.init_hidden(batch_size, x_brand.device)\n",
    "        lstm_out_brand, _ = self.lstm_brand(x_brand, hidden_brand)\n",
    "        last_output_brand = lstm_out_brand[:, -1, :]\n",
    "\n",
    "        # Sales LSTM\n",
    "        hidden_sales = self.init_hidden(batch_size, x_sales.device)\n",
    "        lstm_out_sales, _ = self.lstm_sales(x_sales, hidden_sales)\n",
    "        last_output_sales = lstm_out_sales[:, -1, :]\n",
    "\n",
    "        # Concatenate brand and sales outputs\n",
    "        fused_output = torch.cat((last_output_brand, last_output_sales), dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(fused_output))\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f79f7d",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1802a-35ff-4b43-a1a8-16c8079baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader_brand, val_loader_sales, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():   # 검증 단계이므로 역전파가 필요없음, no_grad()를 통해 메모리 사용량을 줄임\n",
    "        for (X_brand, _), (X_sales, Y_sales) in zip(tqdm(val_loader_brand), val_loader_sales):\n",
    "            X_brand = X_brand.to(device)\n",
    "            X_sales = X_sales.to(device)\n",
    "            Y_sales = Y_sales.to(device)\n",
    "            \n",
    "            output = model(X_brand, X_sales)  # 입력으로 브랜드 키워드 카운트와 판매 데이터를 함께 사용\n",
    "            loss = criterion(output, Y_sales)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73d757-32d5-4868-afbb-1b9f2ea13826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader_brand, train_loader_sales, val_loader_brand, val_loader_sales, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for (X_brand, _), (X_sales, Y_sales) in zip(tqdm(train_loader_brand), train_loader_sales):\n",
    "            X_brand = X_brand.to(device)\n",
    "            X_sales = X_sales.to(device)\n",
    "            Y_sales = Y_sales.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X_brand, X_sales)  # 입력으로 브랜드 키워드 카운트와 판매 데이터를 함께 사용\n",
    "            loss = criterion(output, Y_sales)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        val_loss = validation(model, val_loader_brand, val_loader_sales, criterion, device)\n",
    "        print(f'Epoch [{epoch}/{CFG[\"EPOCHS\"]}] - Train Loss: {np.mean(train_loss):.5f} - Val Loss: {val_loss:.5f}')\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Best model updated')\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83fa73-30d5-489c-852b-d655f76a200c",
   "metadata": {},
   "source": [
    "## Run !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1570b00-a309-4e5e-b53d-5848ba53eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalModel()\n",
    "optimizer = torch.optim.NAdam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])    # Adam optimizer를 사용\n",
    "infer_model = train(model, optimizer, train_loader_brand, train_loader_sales, val_loader_brand, val_loader_sales, device)   # 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(model_path):\n",
    "    loaded_model = MultiModalModel()\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    return loaded_model\n",
    "\n",
    "# model = loadModel('./model/MultiModal.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, PATH):\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "saveModel(infer_model, './model/MultiModal_NAdam_Mish.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b20af7-f5b1-4a7a-8eb9-7dde5bbf3d04",
   "metadata": {},
   "source": [
    "## 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d7ca0-899e-4515-a43e-890549f8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_sales = CustomDataset(test_input_sales, None)\n",
    "test_loader_sales = DataLoader(test_dataset_sales, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset_brand = CustomDataset(test_input_brand, None)\n",
    "test_loader_brand = DataLoader(test_dataset_brand, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f30d4-2b19-479f-89b7-bf5bb2adc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader_brand, test_loader_sales, device):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        for X_brand, X_sales in zip(tqdm(test_loader_brand), test_loader_sales):\n",
    "            X_brand = X_brand.to(device)\n",
    "            X_sales = X_sales.to(device)\n",
    "            \n",
    "            output = model(X_brand, X_sales)\n",
    "            output = output.cpu().numpy()\n",
    "            predictions.extend(output)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76e053-6fd2-44a7-8631-d903e7ffa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(infer_model, test_loader_brand, test_loader_sales, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517978aa-445a-4ece-9217-432682f71230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과를 inverse scaling\n",
    "for idx in range(len(pred)):\n",
    "    pred[idx, :] = pred[idx, :] * (scale_max_dict_sales[idx * SKIP_COUNT] - scale_min_dict_sales[idx* SKIP_COUNT]) + scale_min_dict_sales[idx* SKIP_COUNT]\n",
    "\n",
    "pred = np.round(pred, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa77e-fd03-4539-98fe-563fe2a25121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b50eb-d2d8-4c2d-a5e7-9607220fd794",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c84bb-5dbe-4fb3-aff0-7e229ae29a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db62d9c-b3ad-440a-8cc7-4897b2e4860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.iloc[:,1:] = pred\n",
    "submit.to_csv('./output/MultiModal_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
